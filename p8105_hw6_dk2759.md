p8105\_hw6\_dk2759
================
Darwin Keung
11/25/2018

<http://p8105.com/homework_6.html>

|                                                                                                                                                                                                                                                                                                                                        |
| -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- |
| Homework 6 Context This assignment reinforces ideas in Linear Models.                                                                                                                                                                                                                                                                  |
| \#\# Problem 1                                                                                                                                                                                                                                                                                                                         |
| The *Washington Post* has gathered data on homicides in 50 large U.S. cities and made the data available through a GitHub repository [here](https://github.com/washingtonpost/data-homicides). You can read their accompanying article [here](https://www.washingtonpost.com/graphics/2018/investigations/where-murders-go-unsolved/). |

#### Data import and cleaning

``` r
homicide_df = read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(city_state = str_c(city, ",", " ", state), 
         resolved = as.numeric(disposition == "Closed by arrest"), 
         victim_age = as.numeric(victim_age),
         victim_race = ifelse(victim_race == "White", "White", "Non-White"), 
         victim_race = fct_relevel(victim_race, "White")) %>% 
  filter(!(city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"))) %>% 
  select(city_state, resolved, victim_age, victim_race, victim_sex)
```

    ## Parsed with column specification:
    ## cols(
    ##   uid = col_character(),
    ##   reported_date = col_integer(),
    ##   victim_last = col_character(),
    ##   victim_first = col_character(),
    ##   victim_race = col_character(),
    ##   victim_age = col_character(),
    ##   victim_sex = col_character(),
    ##   city = col_character(),
    ##   state = col_character(),
    ##   lat = col_double(),
    ##   lon = col_double(),
    ##   disposition = col_character()
    ## )

    ## Warning in evalq(as.numeric(victim_age), <environment>): NAs introduced by
    ## coercion

The homicide dataset contains data collected from 50 big US cities over
the a decade. It has 48507 rows and 5 columns. Created city\_state
variable (e.g. “Baltimore, MD”), and a binary variable indicating
whether the homicide is solved. Omitted cities Dallas, TX; Phoenix, AZ;
and Kansas City, MO – these don’t report victim race. Also omitted
Tulsa, AL – this is a data entry mistake. Modified victim\_race to have
categories white and non-white, with white as the reference category.
victim\_age is numeric. The variables also include a unique id, report
date, first and last names of victims, age, sex, race, location (city,
state, lat, long), and disposition.

### glm Baltimore

For the city of Baltimore, MD, use the `glm` function to fit a logistic
regression with resolved vs unresolved as the outcome and victim age,
sex and race (as just defined) as predictors.

``` r
fit_logr_baltimore = 
  homicide_df %>% 
  filter(city_state == "Baltimore, MD") %>% 
  glm(resolved ~ victim_age + victim_race + victim_sex, 
      data = ., 
      family = binomial())
fit_logr_baltimore %>% 
  broom::tidy() %>% 
  janitor::clean_names() %>% 
  filter(term == "victim_raceNon-White") %>% 
  mutate(OR = exp(estimate), 
         lower_95ci = exp(estimate - 1.96*std_error), 
         upper_95ci = exp(estimate + 1.96*std_error)) %>%
  select(OR, lower_95ci, upper_95ci) %>% 
  knitr::kable(digits = 3)
```

|    OR | lower\_95ci | upper\_95ci |
| ----: | ----------: | ----------: |
| 0.441 |       0.313 |        0.62 |

The adjusted odd ratio of resolving the homicide case for non-white
victims compared to white victims after adjusting for age and sex are
0.441 (95% CI: 0.313, 0.620).

### glm for each city

Now run glm for each of the cities in your dataset, and extract the
adjusted odds ratio (and CI) for solving homicides comparing non-white
victims to white victims. Do this within a “tidy” pipeline, making use
of purrr::map, list columns, and unnest as necessary to create a
dataframe with estimated ORs and CIs for each city.

``` r
fit_logr_cities = homicide_df %>% 
  group_by(city_state) %>%
  nest() %>% 
  mutate(models = map(.x = data, ~ glm(resolved ~ victim_sex + victim_race + victim_age, data = .x,
                                      family = binomial)),
         models = map(models, broom::tidy)) %>% 
  select(-data) %>% 
  unnest() %>%
  janitor::clean_names() %>% 
  filter(term == "victim_raceNon-White") %>%
  mutate(OR = exp(estimate), 
         lower_95ci = exp(estimate - 1.96*std_error), 
         upper_95ci = exp(estimate + 1.96*std_error)) %>%
  select(city_state, OR, lower_95ci, upper_95ci)
#table of cities with OR and CI
fit_logr_cities %>% 
  knitr::kable(digits = 3)
```

| city\_state        |    OR | lower\_95ci | upper\_95ci |
| :----------------- | ----: | ----------: | ----------: |
| Albuquerque, NM    | 0.741 |       0.451 |       1.218 |
| Atlanta, GA        | 0.753 |       0.432 |       1.313 |
| Baltimore, MD      | 0.441 |       0.313 |       0.620 |
| Baton Rouge, LA    | 0.668 |       0.313 |       1.425 |
| Birmingham, AL     | 1.039 |       0.615 |       1.756 |
| Boston, MA         | 0.115 |       0.047 |       0.278 |
| Buffalo, NY        | 0.390 |       0.213 |       0.715 |
| Charlotte, NC      | 0.558 |       0.321 |       0.969 |
| Chicago, IL        | 0.562 |       0.431 |       0.733 |
| Cincinnati, OH     | 0.318 |       0.184 |       0.551 |
| Columbus, OH       | 0.855 |       0.634 |       1.152 |
| Denver, CO         | 0.602 |       0.359 |       1.009 |
| Detroit, MI        | 0.651 |       0.488 |       0.869 |
| Durham, NC         | 1.003 |       0.404 |       2.489 |
| Fort Worth, TX     | 0.838 |       0.555 |       1.266 |
| Fresno, CA         | 0.448 |       0.231 |       0.870 |
| Houston, TX        | 0.873 |       0.699 |       1.090 |
| Indianapolis, IN   | 0.505 |       0.382 |       0.667 |
| Jacksonville, FL   | 0.658 |       0.502 |       0.862 |
| Las Vegas, NV      | 0.755 |       0.586 |       0.973 |
| Long Beach, CA     | 0.794 |       0.388 |       1.626 |
| Los Angeles, CA    | 0.666 |       0.483 |       0.918 |
| Louisville, KY     | 0.392 |       0.259 |       0.593 |
| Memphis, TN        | 0.782 |       0.524 |       1.168 |
| Miami, FL          | 0.576 |       0.377 |       0.880 |
| Milwaukee, wI      | 0.632 |       0.403 |       0.991 |
| Minneapolis, MN    | 0.646 |       0.345 |       1.209 |
| Nashville, TN      | 0.899 |       0.653 |       1.236 |
| New Orleans, LA    | 0.466 |       0.295 |       0.737 |
| New York, NY       | 0.531 |       0.279 |       1.011 |
| Oakland, CA        | 0.213 |       0.104 |       0.435 |
| Oklahoma City, OK  | 0.681 |       0.478 |       0.971 |
| Omaha, NE          | 0.169 |       0.094 |       0.305 |
| Philadelphia, PA   | 0.644 |       0.486 |       0.852 |
| Pittsburgh, PA     | 0.282 |       0.161 |       0.493 |
| Richmond, VA       | 0.447 |       0.162 |       1.238 |
| San Antonio, TX    | 0.689 |       0.461 |       1.030 |
| Sacramento, CA     | 0.781 |       0.449 |       1.359 |
| Savannah, GA       | 0.596 |       0.280 |       1.270 |
| San Bernardino, CA | 0.880 |       0.393 |       1.972 |
| San Diego, CA      | 0.483 |       0.298 |       0.785 |
| San Francisco, CA  | 0.458 |       0.290 |       0.723 |
| St. Louis, MO      | 0.577 |       0.406 |       0.820 |
| Stockton, CA       | 0.376 |       0.196 |       0.719 |
| Tampa, FL          | 1.159 |       0.587 |       2.288 |
| Tulsa, OK          | 0.602 |       0.413 |       0.879 |
| Washington, DC     | 0.510 |       0.258 |       1.010 |

Create a plot that shows the estimated ORs and CIs for each city.
Organize cities according to estimated OR, and comment on the plot.

``` r
fit_logr_cities %>% 
  arrange(desc(OR)) %>% 
  mutate(city_state = fct_inorder(city_state))
```

    ## # A tibble: 47 x 4
    ##    city_state            OR lower_95ci upper_95ci
    ##    <fct>              <dbl>      <dbl>      <dbl>
    ##  1 Tampa, FL          1.16       0.587       2.29
    ##  2 Birmingham, AL     1.04       0.615       1.76
    ##  3 Durham, NC         1.00       0.404       2.49
    ##  4 Nashville, TN      0.899      0.653       1.24
    ##  5 San Bernardino, CA 0.880      0.393       1.97
    ##  6 Houston, TX        0.873      0.699       1.09
    ##  7 Columbus, OH       0.855      0.634       1.15
    ##  8 Fort Worth, TX     0.838      0.555       1.27
    ##  9 Long Beach, CA     0.794      0.388       1.63
    ## 10 Memphis, TN        0.782      0.524       1.17
    ## # ... with 37 more rows

``` r
ggplot(fit_logr_cities, aes(x = city_state, y = OR )) + 
  geom_point() + 
  geom_errorbar(aes(ymin = lower_95ci, ymax = upper_95ci)) + 
  geom_hline(aes(yintercept = 1.00), linetype = "dashed", color = "green") +
  coord_flip() +
  theme(text = element_text(size = 9)) +
  labs(
    y = "Estimated Odds Ratio (95% Confidence interval)",
    x = "Location",
    title = "Estimated Odds Ratios of Resolved Homicides for Non-Whites compared to Whites Adjusted for Age and Sex by City"
  )
```

![](p8105_hw6_dk2759_files/figure-gfm/plot_cities-1.png)<!-- -->

Problem 2 In this probelm, you will analyze data gathered to understand
the effects of several variables on a child’s birthweight. This dataset,
available here, consists of roughly 4000 children and includes the
following variables:

babysex: baby’s sex (male = 1, female = 2) bhead: baby’s head
circumference at birth (centimeters) blength: baby’s length at birth
(centimeteres) bwt: baby’s birth weight (grams) delwt: mother’s weight
at delivery (pounds) fincome: family monthly income (in hundreds,
rounded) frace: father’s race (1= White, 2 = Black, 3 = Asian, 4 =
Puerto Rican, 8 = Other, 9 = Unknown) gaweeks: gestational age in weeks
malform: presence of malformations that could affect weight (0 = absent,
1 = present) menarche: mother’s age at menarche (years) mheigth:
mother’s height (inches) momage: mother’s age at delivery (years)
mrace: mother’s race (1= White, 2 = Black, 3 = Asian, 4 = Puerto Rican,
8 = Other) parity: number of live births prior to this pregnancy
pnumlbw: previous number of low birth weight babies pnumgsa: number of
prior small for gestational age babies ppbmi: mother’s pre-pregnancy BMI
ppwt: mother’s pre-pregnancy weight (pounds) smoken: average number of
cigarettes smoked per day during pregnancy wtgain: mother’s weight gain
during pregnancy (pounds) Load and clean the data for regression
analysis (i.e. convert numeric to factor where appropriate, check for
missing data, etc.).

Propose a regression model for birthweight. This model may be based on a
hypothesized structure for the factors that underly birthweight, on a
data-driven model-building process, or a combination of the two.
Describe your modeling process and show a plot of model residuals
against fitted values – use add\_predictions and add\_residuals in
making this plot.

Compare your model to two others:

One using length at birth and gestational age as predictors (main
effects only) One using head circumference, length, sex, and all
interactions (including the three-way interaction) between these Make
this comparison in terms of the cross-validated prediction error; use
crossv\_mc and functions in purrr as appropriate.

Note that although we expect your model to be reasonable, model building
itself is not a main idea of the course and we don’t necessarily expect
your model to be “optimal”.

-----

#### Data import

Describe the raw data:

These data have 48507 rows and 5 columns. The variables include a unique
id, report date, first and last names of victims, age, sex, race,
location (city, state, lat, long), and disposition.

### Binary outcome

Extract data

``` r
baltimore_df = 
  read_csv("https://raw.githubusercontent.com/washingtonpost/data-homicides/master/homicide-data.csv") %>% 
  filter(city == "Baltimore") %>% 
  mutate(resolved = as.numeric(disposition == "Closed by arrest"),
         victim_age = as.numeric(victim_age),
         victim_race = fct_relevel(victim_race, "White")) %>% 
  select(resolved, victim_age, victim_race, victim_sex)
```

    ## Parsed with column specification:
    ## cols(
    ##   uid = col_character(),
    ##   reported_date = col_integer(),
    ##   victim_last = col_character(),
    ##   victim_first = col_character(),
    ##   victim_race = col_character(),
    ##   victim_age = col_character(),
    ##   victim_sex = col_character(),
    ##   city = col_character(),
    ##   state = col_character(),
    ##   lat = col_double(),
    ##   lon = col_double(),
    ##   disposition = col_character()
    ## )

Fit the model

``` r
fit_logistic = 
  glm(resolved ~ victim_age + victim_race, 
    data = baltimore_df, 
    family = binomial())
```

Summarize fit

``` r
summary(fit_logistic)
```

    ## 
    ## Call:
    ## glm(formula = resolved ~ victim_age + victim_race, family = binomial(), 
    ##     data = baltimore_df)
    ## 
    ## Deviance Residuals: 
    ##     Min       1Q   Median       3Q      Max  
    ## -1.4651  -0.9209  -0.8935   1.4467   1.6433  
    ## 
    ## Coefficients:
    ##                      Estimate Std. Error z value Pr(>|z|)    
    ## (Intercept)          0.535470   0.206039   2.599  0.00935 ** 
    ## victim_age          -0.006147   0.003260  -1.886  0.05933 .  
    ## victim_raceAsian     0.242438   0.648110   0.374  0.70835    
    ## victim_raceBlack    -1.026360   0.169652  -6.050 1.45e-09 ***
    ## victim_raceHispanic -0.449746   0.311939  -1.442  0.14937    
    ## victim_raceOther    -1.028579   0.881681  -1.167  0.24337    
    ## ---
    ## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
    ## 
    ## (Dispersion parameter for binomial family taken to be 1)
    ## 
    ##     Null deviance: 3676  on 2826  degrees of freedom
    ## Residual deviance: 3631  on 2821  degrees of freedom
    ## AIC: 3643
    ## 
    ## Number of Fisher Scoring iterations: 4

Summarize fit

``` r
fit_logistic %>% 
  broom::tidy() %>% 
  mutate(OR = boot::inv.logit(estimate)) %>% 
  knitr::kable(digits = 3)
```

| term                 | estimate | std.error | statistic | p.value |    OR |
| :------------------- | -------: | --------: | --------: | ------: | ----: |
| (Intercept)          |    0.535 |     0.206 |     2.599 |   0.009 | 0.631 |
| victim\_age          |  \-0.006 |     0.003 |   \-1.886 |   0.059 | 0.498 |
| victim\_raceAsian    |    0.242 |     0.648 |     0.374 |   0.708 | 0.560 |
| victim\_raceBlack    |  \-1.026 |     0.170 |   \-6.050 |   0.000 | 0.264 |
| victim\_raceHispanic |  \-0.450 |     0.312 |   \-1.442 |   0.149 | 0.389 |
| victim\_raceOther    |  \-1.029 |     0.882 |   \-1.167 |   0.243 | 0.263 |

``` r
baltimore_df %>% 
  ggplot(aes(x = victim_age, y = resolved)) + geom_point() + geom_smooth()
```

    ## `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = "cs")'

![](p8105_hw6_dk2759_files/figure-gfm/unnamed-chunk-4-1.png)<!-- -->
